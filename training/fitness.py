"""
Fitness evaluation through self-play.

Evaluates agent fitness by playing poker hands against other agents.
Fitness = average big blinds won per 100 hands (BB/100).
"""
import numpy as np
from numpy.random import PCG64, Generator
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import multiprocessing as mp
from functools import partial

from .config import FitnessConfig, NetworkConfig
from .genome import Genome, GenomeFactory
from .policy_network import PolicyNetwork, create_action_mask

# Action object cache to avoid repeated object creation
_ACTION_CACHE = {}

# Game object pool for memory reuse
class GamePool:
    """
    Pool of reusable PokerGame objects to reduce allocation overhead.
    Provides ~1.2-1.4× speedup by reusing game objects.
    """
    def __init__(self, pool_size: int = 100):
        self.available = []
        self.pool_size = pool_size
    
    def acquire(self, player_stacks, small_blind, big_blind, ante, seed):
        """Get a game from pool or create new one."""
        if self.available:
            game = self.available.pop()
            # Reset game state
            game.__init__(
                player_stacks=player_stacks,
                small_blind=small_blind,
                big_blind=big_blind,
                ante=ante,
                seed=seed,
                enable_history=False
            )
            return game
        # Create new if pool empty
        from engine import PokerGame
        return PokerGame(
            player_stacks=player_stacks,
            small_blind=small_blind,
            big_blind=big_blind,
            ante=ante,
            seed=seed,
            enable_history=False
        )
    
    def release(self, game):
        """Return game to pool."""
        if len(self.available) < self.pool_size:
            self.available.append(game)

# Global game pool (thread-local would be better for multiprocessing)
_GAME_POOL = GamePool(100)


@dataclass
class EvalResult:
    """Result of evaluating a single genome."""
    genome_id: int
    fitness: float  # BB/100
    total_hands: int
    total_chip_delta: int
    matchups_played: int


def abstract_action_to_engine_action(action_idx: int, game, player_id: int):
    """
    Convert abstract action index to engine Action.
    Optimized with cached action objects for frequently used actions.
    
    Abstract actions:
        0: fold
        1: check/call
        2: raise 0.5x pot
        3: raise 1.0x pot
        4: raise 2.0x pot
        5: all-in
    
    Args:
        action_idx: Abstract action index (0-5)
        game: PokerGame instance
        player_id: Acting player
        
    Returns:
        Action object for the engine
    """
    from engine import Action
    
    # Use cached actions for fold/check/call/allin
    if action_idx == 0:
        if 'fold' not in _ACTION_CACHE:
            _ACTION_CACHE['fold'] = Action('fold')
        return _ACTION_CACHE['fold']
    
    player = game.players[player_id]
    to_call = game.current_bet - player.bet
    
    if action_idx == 1:
        # Check or Call
        if to_call == 0:
            if 'check' not in _ACTION_CACHE:
                _ACTION_CACHE['check'] = Action('check')
            return _ACTION_CACHE['check']
        else:
            if 'call' not in _ACTION_CACHE:
                _ACTION_CACHE['call'] = Action('call')
            return _ACTION_CACHE['call']
    
    pot = game.state.pot.total
    
    if action_idx == 2:
        # Raise 0.5x pot
        raise_amount = max(game.state.big_blind, int(pot * 0.5))
        if player.stack <= to_call + raise_amount:
            if 'allin' not in _ACTION_CACHE:
                _ACTION_CACHE['allin'] = Action('all-in')
            return _ACTION_CACHE['allin']
        return Action('raise', amount=raise_amount)
    
    elif action_idx == 3:
        # Raise 1x pot
        raise_amount = max(game.state.big_blind, pot)
        if player.stack <= to_call + raise_amount:
            if 'allin' not in _ACTION_CACHE:
                _ACTION_CACHE['allin'] = Action('all-in')
            return _ACTION_CACHE['allin']
        return Action('raise', amount=raise_amount)
    
    elif action_idx == 4:
        # Raise 2x pot
        raise_amount = max(game.state.big_blind, pot * 2)
        if player.stack <= to_call + raise_amount:
            if 'allin' not in _ACTION_CACHE:
                _ACTION_CACHE['allin'] = Action('all-in')
            return _ACTION_CACHE['allin']
        return Action('raise', amount=raise_amount)
    
    elif action_idx == 5:
        # All-in
        if 'allin' not in _ACTION_CACHE:
            _ACTION_CACHE['allin'] = Action('all-in')
        return _ACTION_CACHE['allin']
    
    else:
        # Default to fold for invalid actions
        return Action('fold')


def play_hands_batched(networks_list: List[List[PolicyNetwork]], 
                       games: List,
                       rng: np.random.Generator,
                       temperature: float = 1.0) -> List[Dict[int, int]]:
    """
    Play multiple hands in parallel with batched neural network inference.
    Provides 1.3-1.5× speedup by processing multiple decisions simultaneously.
    
    Args:
        networks_list: List of network lists (one list per game)
        games: List of PokerGame instances
        rng: Random number generator
        temperature: Action sampling temperature
        
    Returns:
        List of chip change dicts (one per game)
    """
    from engine.features import FeatureCache
    
    batch_size = len(games)
    stacks_before = [[p.stack for p in game.players] for game in games]
    max_actions = 200
    
    # Track state for each game
    game_states = []
    for idx, game in enumerate(games):
        feature_caches = [FeatureCache(game, i) for i in range(len(game.players))]
        game_states.append({
            'game': game,
            'networks': networks_list[idx],
            'feature_caches': feature_caches,
            'action_count': 0,
            'finished': False
        })
    
    # Play all games step by step, batching decisions across games
    while any(not gs['finished'] for gs in game_states):
        # Collect current decisions from all active games
        batch_features = []
        batch_masks = []
        batch_info = []  # (game_state_idx, current_player, network)
        
        for gs_idx, gs in enumerate(game_states):
            if gs['finished']:
                continue
                
            game = gs['game']
            if game.is_hand_over() or gs['action_count'] >= max_actions:
                gs['finished'] = True
                continue
            
            current = game.state.current_player
            if current is None:
                gs['finished'] = True
                continue
            
            player = game.players[current]
            if player.has_folded or player.is_all_in:
                gs['finished'] = True
                continue
            
            # Collect features and mask for this decision
            features = gs['feature_caches'][current].get_features(game)
            mask = create_action_mask(game, current)
            
            batch_features.append(features)
            batch_masks.append(mask)
            batch_info.append((gs_idx, current, gs['networks'][current]))
        
        # If no active decisions, we're done
        if len(batch_features) == 0:
            break
        
        # Batch inference: process all decisions at once
        if len(batch_features) == 1:
            # Single decision - use regular select_action
            action_idx = batch_info[0][2].select_action(
                batch_features[0], batch_masks[0], rng, temperature
            )
            action_indices = [action_idx]
        else:
            # Multiple decisions - use batched processing
            features_array = np.array(batch_features, dtype=np.float32)
            masks_array = np.array(batch_masks, dtype=np.float32)
            
            # Use first network's select_action_batch (all networks same architecture)
            action_indices = batch_info[0][2].select_action_batch(
                features_array, masks_array, rng, temperature
            )
        
        # Apply actions to respective games
        for i, (gs_idx, current, network) in enumerate(batch_info):
            gs = game_states[gs_idx]
            game = gs['game']
            action_idx = action_indices[i]
            
            # Convert to engine action
            action = abstract_action_to_engine_action(action_idx, game, current)
            
            # Apply action
            try:
                game.apply_action(current, action)
            except Exception:
                # If action fails, try fold
                try:
                    from engine import Action
                    game.apply_action(current, Action('fold'))
                except:
                    gs['finished'] = True
            
            gs['action_count'] += 1
    
    # Resolve showdowns and calculate results
    results = []
    for idx, gs in enumerate(game_states):
        game = gs['game']
        
        if game.state.betting_round == 'showdown':
            try:
                game.resolve_showdown()
            except:
                pass
        
        # Calculate chip changes
        changes = {}
        for i, player in enumerate(game.players):
            changes[i] = player.stack - stacks_before[idx][i]
        results.append(changes)
    
    return results


def play_hand(networks: List[PolicyNetwork], game,
              rng: np.random.Generator,
              temperature: float = 1.0) -> Dict[int, int]:
    """
    Play a single hand and return chip changes.
    
    Args:
        networks: List of policy networks (one per player)
        game: PokerGame instance (already initialized)
        rng: Random number generator
        temperature: Action sampling temperature
        
    Returns:
        Dict mapping player_id to chip change
    """
    from engine.features import FeatureCache
    
    stacks_before = [p.stack for p in game.players]
    max_actions = 200  # Safety limit
    action_count = 0
    
    # Create feature caches once per hand (1.5-2× speedup)
    feature_caches = [FeatureCache(game, i) for i in range(len(game.players))]
    
    while not game.is_hand_over() and action_count < max_actions:
        current = game.state.current_player
        if current is None:
            break
        
        player = game.players[current]
        if player.has_folded or player.is_all_in:
            # This shouldn't happen but safety check
            break
        
        # Get state features (using cached static features) and action mask
        features = feature_caches[current].get_features(game)
        mask = create_action_mask(game, current)
        
        # Select action from network
        action_idx = networks[current].select_action(features, mask, rng, temperature)
        
        # Convert to engine action
        action = abstract_action_to_engine_action(action_idx, game, current)
        
        # Apply action
        try:
            game.apply_action(current, action)
        except Exception as e:
            # If action fails, try fold
            try:
                from engine import Action
                game.apply_action(current, Action('fold'))
            except:
                break
        
        action_count += 1
    
    # Resolve showdown if needed
    if game.state.betting_round == 'showdown':
        try:
            game.resolve_showdown()
        except:
            pass
    
    # Calculate chip changes
    changes = {}
    for i, player in enumerate(game.players):
        changes[i] = player.stack - stacks_before[i]
    
    return changes


def evaluate_matchup(genome_weights: np.ndarray,
                    opponent_weights: List[np.ndarray],
                    network_config: NetworkConfig,
                    fitness_config: FitnessConfig,
                    seed: int,
                    hand_seeds: Optional[List[int]] = None) -> Tuple[int, int]:
    """
    Evaluate one matchup (hero vs opponents over many hands).
    
    Args:
        genome_weights: Hero genome weights
        opponent_weights: List of opponent weights
        network_config: Network config
        fitness_config: Evaluation config
        seed: Random seed for reproducibility
        
    Returns:
        Tuple of (total_chip_delta, hands_played)
    """
    from engine import PokerGame
    
    # Use PCG64 for faster random number generation
    rng = Generator(PCG64(seed))
    # Create networks
    hero_net = PolicyNetwork(network_config)
    hero_net.set_weights_from_genome(genome_weights)
    opponent_nets = []
    for w in opponent_weights:
        net = PolicyNetwork(network_config)
        net.set_weights_from_genome(w)
        opponent_nets.append(net)
    num_players = fitness_config.num_players
    # Create network list and randomize seat positions
    networks = [hero_net] + opponent_nets[:num_players - 1]
    while len(networks) < num_players:
        networks.append(opponent_nets[rng.integers(len(opponent_nets))])

    # For each hand, shuffle seat positions
    # Prepare hand seeds
    num_hands = fitness_config.hands_per_matchup
    if hand_seeds is None:
        # Use random hands for training
        hand_seeds = [int(rng.integers(0, 2**31)) for _ in range(num_hands)]
    # Vary stack sizes and blind levels for each hand
    stack_min = int(getattr(fitness_config, 'stack_min', fitness_config.starting_stack * 0.5))
    stack_max = int(getattr(fitness_config, 'stack_max', fitness_config.starting_stack * 1.5))
    sb_min = int(getattr(fitness_config, 'sb_min', fitness_config.small_blind))
    sb_max = int(getattr(fitness_config, 'sb_max', fitness_config.small_blind * 2))
    bb_min = int(getattr(fitness_config, 'bb_min', fitness_config.big_blind))
    bb_max = int(getattr(fitness_config, 'bb_max', fitness_config.big_blind * 2))
    ante_min = int(getattr(fitness_config, 'ante_min', fitness_config.ante))
    ante_max = int(getattr(fitness_config, 'ante_max', max(1, fitness_config.ante * 2)))

    def new_game(hand_seed, seat_order):
        # Randomize stacks and blinds for each hand
        stacks = [int(rng.integers(stack_min, stack_max+1)) for _ in range(num_players)]
        stacks = [stacks[i] for i in seat_order]
        sb = int(rng.integers(sb_min, sb_max+1))
        bb = int(rng.integers(bb_min, bb_max+1))
        ante = int(rng.integers(ante_min, ante_max+1)) if ante_max > 0 else 0
        # Use game pool for memory reuse
        return _GAME_POOL.acquire(
            player_stacks=stacks,
            small_blind=sb,
            big_blind=bb,
            ante=ante,
            seed=hand_seed
        )
    total_delta = 0
    hands_played = 0
    
    # Process hands in batches for better performance
    batch_size = 8  # Process 8 hands simultaneously
    
    for batch_start in range(0, num_hands, batch_size):
        batch_end = min(batch_start + batch_size, num_hands)
        batch_hands = batch_end - batch_start
        
        # Prepare batch of games
        games_batch = []
        networks_batch = []
        seat_orders_batch = []
        
        for hand_idx in range(batch_start, batch_end):
            # Shuffle seat positions
            seat_order = list(range(num_players))
            rng.shuffle(seat_order)
            seat_orders_batch.append(seat_order)
            
            # Shuffle networks to match seat order
            shuffled_networks = [networks[i] for i in seat_order]
            networks_batch.append(shuffled_networks)
            
            # Create game
            game = new_game(hand_seeds[hand_idx], seat_order)
            games_batch.append(game)
        
        # Play batch of hands with batched inference
        if batch_hands == 1:
            # Single hand - use regular play_hand
            changes_batch = [play_hand(networks_batch[0], games_batch[0], rng, fitness_config.temperature)]
        else:
            # Multiple hands - use batched processing
            changes_batch = play_hands_batched(networks_batch, games_batch, rng, fitness_config.temperature)
        
        # Accumulate results
        for changes in changes_batch:
            total_delta += changes.get(0, 0)
            hands_played += 1
        
        # Return games to pool
        for game in games_batch:
            _GAME_POOL.release(game)
            
            # Check for busted players - reset stacks if needed
            active = [p for p in game.players if p.stack > 0]
            if len(active) < 2:
                for i, p in enumerate(game.players):
                    game.players[i].stack = fitness_config.starting_stack
                game.players[i].stack = fitness_config.starting_stack
    
    return total_delta, hands_played


def _worker_evaluate_genome_with_hof(args: Tuple) -> Dict:
    """
    Worker function for parallel evaluation with HOF tracking.
    
    Args:
        args: Tuple of (genome_id, genome_weights, opponent_weights_list, 
                       network_config, fitness_config, base_seed, hof_info)
                       
    Returns:
        Dict with evaluation results including HOF usage
    """
    if len(args) == 7:
        # New format with HOF tracking
        genome_id, genome_weights, opponent_weights_list, network_config, fitness_config, base_seed, hof_info = args
    else:
        # Old format for backward compatibility  
        genome_id, genome_weights, opponent_weights_list, network_config, fitness_config, base_seed = args
        hof_info = {}
    
    # Call the original worker function
    original_args = (genome_id, genome_weights, opponent_weights_list, network_config, fitness_config, base_seed, hof_info)
    result = _worker_evaluate_genome(original_args)
    
    # Convert EvalResult to dict and add HOF tracking
    result_dict = {
        'genome_id': genome_id,
        'fitness': result.fitness,
        'num_hands': result.num_hands,
        'win_rate': result.win_rate,
        'num_matchups': result.num_matchups
    }
    
    # Add HOF tracking information
    if hof_info and 'hof_ids_used' in hof_info:
        result_dict['hof_ids_used'] = hof_info['hof_ids_used']
        result_dict['hof_count_per_matchup'] = hof_info['hof_count_per_matchup']
        result_dict['total_hof_opponents'] = len(hof_info['hof_ids_used'])
    else:
        result_dict['hof_ids_used'] = []
        result_dict['hof_count_per_matchup'] = []
        result_dict['total_hof_opponents'] = 0
    
    return result_dict


def _worker_evaluate_genome(args: Tuple) -> EvalResult:
    """
    Worker function for parallel evaluation.
    
    Args:
        args: Tuple of (genome_id, genome_weights, opponent_weights_list, 
                       network_config, fitness_config, base_seed, hof_info)
                       
    Returns:
        EvalResult for this genome
    """
    (genome_id, genome_weights, opponent_weights_list,
     network_config, fitness_config, base_seed, hof_info) = args
    
    total_delta = 0
    total_hands = 0
    for matchup_idx, opponents in enumerate(opponent_weights_list):
        seed = base_seed + genome_id * 1000 + matchup_idx
        # For training, use random hands (hand_seeds=None)
        delta, hands = evaluate_matchup(
            genome_weights, opponents,
            network_config, fitness_config, seed,
            hand_seeds=None
        )
        total_delta += delta
        total_hands += hands
    # Calculate BB/100
    bb = fitness_config.big_blind
    bb_per_100 = (total_delta / bb) * (100 / max(1, total_hands))
    return EvalResult(
        genome_id=genome_id,
        fitness=bb_per_100,
        total_hands=total_hands,
        total_chip_delta=total_delta,
        matchups_played=len(opponent_weights_list),
    )

def evaluate_fixed_hands(genome_weights: np.ndarray,
                        opponent_weights: List[np.ndarray],
                        network_config: NetworkConfig,
                        fitness_config: FitnessConfig,
                        eval_hand_seeds: List[int],
                        seed: int) -> Tuple[int, int]:
    """
    Evaluate a genome on a fixed set of hands for fair comparison.
    """
    return evaluate_matchup(
        genome_weights, opponent_weights,
        network_config, fitness_config, seed,
        hand_seeds=eval_hand_seeds
    )
    
    # Calculate BB/100
    bb = fitness_config.big_blind
    bb_per_100 = (total_delta / bb) * (100 / max(1, total_hands))
    
    return EvalResult(
        genome_id=genome_id,
        fitness=bb_per_100,
        total_hands=total_hands,
        total_chip_delta=total_delta,
        matchups_played=len(opponent_weights_list),
    )


class FitnessEvaluator:
    """
    Evaluates genome fitness through self-play.
    
    Each genome plays as "hero" against various opponent configurations.
    Opponents are sampled from:
        1. Other genomes in current population
        2. Hall of fame (historical good agents)
        3. Random agents (for diversity)
    
    Fitness = average BB/100 across all matchups.
    """
    
    def __init__(self, factory: GenomeFactory,
                 config: FitnessConfig,
                 rng: Optional[np.random.Generator] = None):
        """
        Initialize evaluator.
        
        Args:
            factory: GenomeFactory for creating networks
            config: Fitness evaluation config
            rng: Random number generator
        """
        self.factory = factory
        self.config = config
        self.rng = rng or np.random.default_rng()
    
    def create_opponent_groups(self, genomes: List[Genome],
                               hall_of_fame: Optional[List[Genome]] = None,
                               hof_max_size: int = 20) -> Tuple[List[List[np.ndarray]], List[List[int]]]:
        """
        Create opponent weight groups for evaluation.
        
        Each group has (num_players - 1) opponent weights.
        
        Args:
            genomes: Current population
            hall_of_fame: Optional historical best agents
            
        Returns:
            Tuple of (opponent weight lists, HOF ID tracking lists)
        """
        num_opponents = self.config.num_players - 1
        num_matchups = self.config.matchups_per_agent
        
        # Collect all potential opponent weights
        all_weights = [g.weights for g in genomes]
        
        hof_weights = []
        hof_genome_ids = []
        if hall_of_fame:
            # Keep only the most diverse and highest-performing agents in the Hall of Fame
            hof_sorted = sorted(hall_of_fame, key=lambda g: g.fitness if g.fitness is not None else -1, reverse=True)
            hof_selected = []
            for g in hof_sorted:
                # Add if sufficiently different from those already selected
                if not hof_selected:
                    hof_selected.append(g)
                else:
                    dists = [np.linalg.norm(g.weights - h.weights) for h in hof_selected]
                    if min(dists) > 0.1:  # Diversity threshold (tune as needed)
                        hof_selected.append(g)
                if len(hof_selected) >= hof_max_size:
                    break
            hof_weights = [g.weights for g in hof_selected]
            hof_genome_ids = [g.genome_id for g in hof_selected]
        
        groups = []
        hof_tracking = []  # Track which HOF members are used in each group
        
        for _ in range(num_matchups):
            group = []
            group_hof_ids = []  # HOF IDs used in this group
            
            for _ in range(num_opponents):
                # Decide source: population, HoF, or random
                r = self.rng.random()
                
                if r < 0.2 and hof_weights:
                    # 20% from HoF
                    idx = self.rng.integers(len(hof_weights))
                    group.append(hof_weights[idx])
                    group_hof_ids.append(hof_genome_ids[idx])
                elif r < 0.3:
                    # 10% random
                    random_weights = self.rng.standard_normal(
                        self.factory.genome_size
                    ).astype(np.float32) * 0.1
                    group.append(random_weights)
                    # No HOF ID for random opponents
                else:
                    # 70% from population
                    idx = self.rng.integers(len(all_weights))
                    group.append(all_weights[idx])
                    # No HOF ID for population opponents
            
            groups.append(group)
            hof_tracking.append(group_hof_ids)
        
        return groups, hof_tracking
    
    def evaluate_single(self, genome: Genome,
                       opponents: List[Genome],
                       num_hands: Optional[int] = None) -> float:
        """
        Evaluate a single genome against opponents.
        
        Args:
            genome: Genome to evaluate
            opponents: Opponent genomes
            num_hands: Override hands per evaluation
            
        Returns:
            BB/100 fitness score
        """
        if num_hands is not None:
            old_hands = self.config.hands_per_matchup
            self.config.hands_per_matchup = num_hands // self.config.matchups_per_agent
        
        opponent_groups, _ = self.create_opponent_groups(opponents)
        
        total_delta = 0
        total_hands = 0
        
        for matchup_idx, opponent_weights in enumerate(opponent_groups):
            seed = self.rng.integers(0, 2**31)
            delta, hands = evaluate_matchup(
                genome.weights, opponent_weights,
                self.factory.network_config,
                self.config, seed
            )
            total_delta += delta
            total_hands += hands
        
        if num_hands is not None:
            self.config.hands_per_matchup = old_hands
        
        bb = self.config.big_blind
        return (total_delta / bb) * (100 / max(1, total_hands))
    
    def evaluate_population(self, genomes: List[Genome],
                           hall_of_fame: Optional[List[Genome]] = None,
                           parallel: bool = False,
                           track_hof_usage: bool = True) -> Dict[int, EvalResult]:
        """
        Evaluate fitness for all genomes in population.
        
        Args:
            genomes: List of genomes to evaluate
            hall_of_fame: Optional HoF for opponent diversity
            parallel: Use multiprocessing
            track_hof_usage: Whether to track which HOF members were used as opponents
            
        Returns:
            Dict mapping genome_id to EvalResult
        """
        # Create opponent groups with optional HOF tracking
        if track_hof_usage:
            opponent_groups, hof_tracking = self.create_opponent_groups(genomes, hall_of_fame)
        else:
            # Fallback for backward compatibility
            result = self.create_opponent_groups(genomes, hall_of_fame)
            if isinstance(result, tuple):
                opponent_groups, hof_tracking = result
            else:
                opponent_groups = result
                hof_tracking = [[] for _ in opponent_groups]
        
        # Prepare evaluation arguments
        base_seed = self.rng.integers(0, 2**31)
        args_list = []
        for i, g in enumerate(genomes):
            # Include HOF tracking info for each genome
            hof_info = {
                'hof_ids_used': [hof_id for group_hof in hof_tracking for hof_id in group_hof],
                'hof_count_per_matchup': [len(group_hof) for group_hof in hof_tracking]
            } if track_hof_usage else {}
            
            args_list.append((
                g.genome_id, g.weights, opponent_groups,
                self.factory.network_config, self.config, base_seed, hof_info
            ))
        
        if parallel and self.config.num_workers > 1:
            # Parallel evaluation
            with mp.Pool(self.config.num_workers) as pool:
                results = pool.map(_worker_evaluate_genome, args_list)
        else:
            # Sequential evaluation
            results = [_worker_evaluate_genome(args) for args in args_list]
        
        # Update genome fitness and build result dict
        result_dict = {}
        for result in results:
            result_dict[result.genome_id] = result
            
            # Update genome fitness
            for g in genomes:
                if g.genome_id == result.genome_id:
                    g.fitness = result.fitness
                    break
        
        return result_dict
    
    def evaluate_against_baseline(self, genome: Genome,
                                  num_hands: int = 5000) -> float:
        """
        Evaluate genome against random baseline agents.
        
        Args:
            genome: Genome to evaluate
            num_hands: Total hands to play
            
        Returns:
            BB/100 against random opponents
        """
        # Create random opponent genomes
        random_opponents = []
        for _ in range(self.config.num_players - 1):
            random_weights = self.rng.standard_normal(
                self.factory.genome_size
            ).astype(np.float32) * 0.1
            random_opponents.append(random_weights)
        
        # Single matchup with all hands
        seed = self.rng.integers(0, 2**31)
        
        # Temporarily increase hands
        old_hands = self.config.hands_per_matchup
        self.config.hands_per_matchup = num_hands
        
        delta, hands = evaluate_matchup(
            genome.weights, random_opponents,
            self.factory.network_config,
            self.config, seed
        )
        
        self.config.hands_per_matchup = old_hands
        
        bb = self.config.big_blind
        return (delta / bb) * (100 / max(1, hands))
